{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6751e26",
   "metadata": {},
   "source": [
    "파인튜닝\n",
    "```\n",
    "LLM을 추가 학습\n",
    "소량의 고품질데이터로 모델을 재조정해서 작업별 성능 향상상\n",
    "```\n",
    "파인튜닝VS프롬프트엔지니얼이\n",
    "```\n",
    "프롬프트엔지니어링\n",
    "    모델의 가중치를 변경하지않고 입력프롬프트를 조정해 원하는 출력을 유도    \n",
    "```\n",
    "파인튜닝\n",
    "```\n",
    "    모델의 가중치를 업데이트 특정 데이터셋에 최적화\n",
    "    종류:\n",
    "        Full File-Tuning : 모델의 모든 가중치를 튜닝\n",
    "        PEFT(Parameter-Effecitve Fine-Turning) :소수의 파라메터만 업데이트 효율성 극대화화\n",
    "            LoRA, QLoRA\n",
    "```\n",
    "LoRA(Low-Rank Adaptation), QLoRA(Quantized LoRA)\n",
    "```\n",
    "    LoRA : 모델의 가중치 행렬에 소규모 추가 파라메터(저랭크 행렬)를 삽입하여 학습\n",
    "    원본모델은유지, 어뎁터만 저장/배포\n",
    "    Llama-3를 한국어 데어터로 LoRA파인튜이\n",
    "\n",
    "    QLoRA : LoRA에 4비트 또는 8비트 양자를 적용 메모리사용량 절감감\n",
    "```\n",
    "한국어 LLM파인튜닝\n",
    "```\n",
    "    영어중심의 LLM 한국어 데이터가 부족\n",
    "    토크나이져가 한국어에 최적화 안됨\n",
    "    해결 : 한국어 특화모델(KoGPT2. Llama-3-Open-Ko, Mistal-Ko)\n",
    "        한국어 데이터셋 이용(KoAlpaca, NIA한국어 대회 데이터셋)\n",
    "        토크나이져 재학습, 한국어 전용 토크나이져져\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876218d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
